# ğŸ“„ PDF-RAG: Local Question Answering Over PDFs

This project is a **local Retrieval-Augmented Generation (RAG) application** that allows users to upload a PDF file and ask questions about its content. It uses **LangChain**, **Streamlit**, **Chroma vector database**, **HuggingFace embeddings**, and **Ollama** for running large language models locally.

---

## ğŸš€ Features

- ğŸ“‚ Upload any PDF file via a simple web interface  
- âœ‚ï¸ Automatically splits PDF text into chunks  
- ğŸ§  Generates embeddings using HuggingFace models  
- ğŸ—„ï¸ Stores embeddings locally using Chroma  
- ğŸ¤– Uses Ollama to run LLMs **fully locally**  
- ğŸ” Retrieves relevant PDF sections to answer questions  
- ğŸ“Œ Displays source pages for transparency  

---

## ğŸ—ï¸ Architecture Overview

1. **PDF Upload (Streamlit Sidebar)**
2. **Document Loading** using `PyPDFLoader`
3. **Text Chunking** with `RecursiveCharacterTextSplitter`
4. **Embedding Generation** via HuggingFace
5. **Vector Storage** using Chroma
6. **Question Answering** with LangChain `RetrievalQA`
7. **LLM Inference** using Ollama (e.g., `llama3.2`)

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit  
- **LLM Framework**: LangChain  
- **Vector Database**: Chroma  
- **Embeddings**: HuggingFace Embeddings  
- **LLM Runtime**: Ollama  
- **Language**: Python  

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/aniragn/RAG_App.git
cd RAG_App